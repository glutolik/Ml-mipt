{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Lasagne implementation of CIFAR-10 examples from \"Deep Residual Learning for Image Recognition\" (http://arxiv.org/abs/1512.03385)\n",
    "\n",
    "Check the accompanying files for pretrained models. The 32-layer network (n=5), achieves a validation error of 7.42%, \n",
    "while the 56-layer network (n=9) achieves error of 6.75%, which is roughly equivalent to the examples in the paper.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "# for the larger networks (n>=9), we need to adjust pythons recursion limit\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##################### Load data from CIFAR-10 dataset #######################\n",
    "# this code assumes the cifar dataset from 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "# has been extracted in current working directory\n",
    "\n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def load_data():\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for j in range(5):\n",
    "      d = unpickle('cifar-10-batches-py/data_batch_'+`j+1`)\n",
    "      x = d['data']\n",
    "      y = d['labels']\n",
    "      xs.append(x)\n",
    "      ys.append(y)\n",
    "\n",
    "    d = unpickle('cifar-10-batches-py/test_batch')\n",
    "    xs.append(d['data'])\n",
    "    ys.append(d['labels'])\n",
    "\n",
    "    x = np.concatenate(xs)/np.float32(255)\n",
    "    y = np.concatenate(ys)\n",
    "    x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3)).transpose(0,3,1,2)\n",
    "\n",
    "    # subtract per-pixel mean\n",
    "    pixel_mean = np.mean(x[0:50000],axis=0)\n",
    "    #pickle.dump(pixel_mean, open(\"cifar10-pixel_mean.pkl\",\"wb\"))\n",
    "    x -= pixel_mean\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "    X_train_flip = X_train[:,:,:,::-1]\n",
    "    Y_train_flip = Y_train\n",
    "    X_train = np.concatenate((X_train,X_train_flip),axis=0)\n",
    "    Y_train = np.concatenate((Y_train,Y_train_flip),axis=0)\n",
    "\n",
    "    X_test = x[50000:,:,:,:]\n",
    "    Y_test = y[50000:]\n",
    "\n",
    "    return dict(\n",
    "        X_train=lasagne.utils.floatX(X_train),\n",
    "        Y_train=Y_train.astype('int32'),\n",
    "        X_test = lasagne.utils.floatX(X_test),\n",
    "        Y_test = Y_test.astype('int32'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##################### Build the neural network model #######################\n",
    "\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "#from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.layers import ElemwiseSumLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import GlobalPoolLayer\n",
    "from lasagne.layers import PadLayer\n",
    "from lasagne.layers import ExpressionLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.nonlinearities import softmax, rectify\n",
    "from lasagne.layers import batch_norm\n",
    "\n",
    "def build_cnn(input_var=None, n=5):\n",
    "    \n",
    "    # create a residual learning building block with two stacked 3x3 convlayers as in paper\n",
    "    def residual_block(l, increase_dim=False, projection=False):\n",
    "        input_num_filters = l.output_shape[1]\n",
    "        if increase_dim:\n",
    "            first_stride = (2,2)\n",
    "            out_num_filters = input_num_filters*2\n",
    "        else:\n",
    "            first_stride = (1,1)\n",
    "            out_num_filters = input_num_filters\n",
    "\n",
    "        stack_1 = batch_norm(ConvLayer(l, num_filters=out_num_filters, filter_size=(3,3), stride=first_stride, nonlinearity=rectify, pad='same', W=lasagne.init.HeNormal(gain='relu'), flip_filters=False))\n",
    "        stack_2 = batch_norm(ConvLayer(stack_1, num_filters=out_num_filters, filter_size=(3,3), stride=(1,1), nonlinearity=None, pad='same', W=lasagne.init.HeNormal(gain='relu'), flip_filters=False))\n",
    "        \n",
    "        # add shortcut connections\n",
    "        if increase_dim:\n",
    "            if projection:\n",
    "                # projection shortcut, as option B in paper\n",
    "                projection = batch_norm(ConvLayer(l, num_filters=out_num_filters, filter_size=(1,1), stride=(2,2), nonlinearity=None, pad='same', b=None, flip_filters=False))\n",
    "                block = NonlinearityLayer(ElemwiseSumLayer([stack_2, projection]),nonlinearity=rectify)\n",
    "            else:\n",
    "                # identity shortcut, as option A in paper\n",
    "                identity = ExpressionLayer(l, lambda X: X[:, :, ::2, ::2], lambda s: (s[0], s[1], s[2]//2, s[3]//2))\n",
    "                padding = PadLayer(identity, [out_num_filters//4,0,0], batch_ndim=1)\n",
    "                block = NonlinearityLayer(ElemwiseSumLayer([stack_2, padding]),nonlinearity=rectify)\n",
    "        else:\n",
    "            block = NonlinearityLayer(ElemwiseSumLayer([stack_2, l]),nonlinearity=rectify)\n",
    "        \n",
    "        return block\n",
    "\n",
    "    # Building the network\n",
    "    l_in = InputLayer(shape=(None, 3, 32, 32), input_var=input_var)\n",
    "\n",
    "    # first layer, output is 16 x 32 x 32\n",
    "    l = batch_norm(ConvLayer(l_in, num_filters=16, filter_size=(3,3), stride=(1,1), nonlinearity=rectify, pad='same', W=lasagne.init.HeNormal(gain='relu'), flip_filters=False))\n",
    "    \n",
    "    # first stack of residual blocks, output is 16 x 32 x 32\n",
    "    for _ in range(n):\n",
    "        l = residual_block(l)\n",
    "\n",
    "    # second stack of residual blocks, output is 32 x 16 x 16\n",
    "    l = residual_block(l, increase_dim=True)\n",
    "    for _ in range(1,n):\n",
    "        l = residual_block(l)\n",
    "\n",
    "    # third stack of residual blocks, output is 64 x 8 x 8\n",
    "    l = residual_block(l, increase_dim=True)\n",
    "    for _ in range(1,n):\n",
    "        l = residual_block(l)\n",
    "    \n",
    "    # average pooling\n",
    "    l = GlobalPoolLayer(l)\n",
    "\n",
    "    # fully connected layer\n",
    "    network = DenseLayer(\n",
    "            l, num_units=10,\n",
    "            W=lasagne.init.HeNormal(),\n",
    "            nonlinearity=softmax)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############################# Batch iterator ###############################\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False, augment=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        if augment:\n",
    "            # as in paper : \n",
    "            # pad feature arrays with 4 pixels on each side\n",
    "            # and do random cropping of 32x32\n",
    "            padded = np.pad(inputs[excerpt],((0,0),(0,0),(4,4),(4,4)),mode='constant')\n",
    "            random_cropped = np.zeros(inputs[excerpt].shape, dtype=np.float32)\n",
    "            crops = np.random.random_integers(0,high=8,size=(batchsize,2))\n",
    "            for r in range(batchsize):\n",
    "                random_cropped[r,:,:,:] = padded[r,:,crops[r,0]:(crops[r,0]+32),crops[r,1]:(crops[r,1]+32)]\n",
    "            inp_exc = random_cropped\n",
    "        else:\n",
    "            inp_exc = inputs[excerpt]\n",
    "\n",
    "        yield inp_exc, targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############################## Main program ################################\n",
    "\n",
    "def main(n=5, num_epochs=82, model=None):\n",
    "    # Check if cifar data exists\n",
    "    if not os.path.exists(\"./cifar-10-batches-py\"):\n",
    "        print(\"CIFAR-10 dataset can not be found. Please download the dataset from 'https://www.cs.toronto.edu/~kriz/cifar.html'.\")\n",
    "        return\n",
    "\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data()\n",
    "    X_train = data['X_train']\n",
    "    Y_train = data['Y_train']\n",
    "    X_test = data['X_test']\n",
    "    Y_test = data['Y_test']\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    network = build_cnn(input_var, n)\n",
    "    print(\"number of parameters in model: %d\" % lasagne.layers.count_params(network, trainable=True))\n",
    "    \n",
    "    if model is None:\n",
    "        # Create a loss expression for training, i.e., a scalar objective we want\n",
    "        # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "        prediction = lasagne.layers.get_output(network)\n",
    "        loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "        loss = loss.mean()\n",
    "        # add weight decay\n",
    "        all_layers = lasagne.layers.get_all_layers(network)\n",
    "        l2_penalty = lasagne.regularization.regularize_layer_params(all_layers, lasagne.regularization.l2) * 0.0001\n",
    "        loss = loss + l2_penalty\n",
    "\n",
    "        # Create update expressions for training\n",
    "        # Stochastic Gradient Descent (SGD) with momentum\n",
    "        params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "        lr = 0.1\n",
    "        sh_lr = theano.shared(lasagne.utils.floatX(lr))\n",
    "        updates = lasagne.updates.momentum(\n",
    "                loss, params, learning_rate=sh_lr, momentum=0.9)\n",
    "        \n",
    "        # Compile a function performing a training step on a mini-batch (by giving\n",
    "        # the updates dictionary) and returning the corresponding training loss:\n",
    "        train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "    # Create a loss expression for validation/testing\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    if model is None:\n",
    "        # launch the training loop\n",
    "        print(\"Starting training...\")\n",
    "        # We iterate over epochs:\n",
    "        for epoch in range(num_epochs):\n",
    "            # shuffle training data\n",
    "            train_indices = np.arange(100000)\n",
    "            np.random.shuffle(train_indices)\n",
    "            X_train = X_train[train_indices,:,:,:]\n",
    "            Y_train = Y_train[train_indices]\n",
    "\n",
    "            # In each epoch, we do a full pass over the training data:\n",
    "            train_err = 0\n",
    "            train_batches = 0\n",
    "            start_time = time.time()\n",
    "            for batch in iterate_minibatches(X_train, Y_train, 128, shuffle=True, augment=True):\n",
    "                inputs, targets = batch\n",
    "                train_err += train_fn(inputs, targets)\n",
    "                train_batches += 1\n",
    "\n",
    "            # And a full pass over the validation data:\n",
    "            val_err = 0\n",
    "            val_acc = 0\n",
    "            val_batches = 0\n",
    "            for batch in iterate_minibatches(X_test, Y_test, 500, shuffle=False):\n",
    "                inputs, targets = batch\n",
    "                err, acc = val_fn(inputs, targets)\n",
    "                val_err += err\n",
    "                val_acc += acc\n",
    "                val_batches += 1\n",
    "\n",
    "            # Then we print the results for this epoch:\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch + 1, num_epochs, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "            print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "            print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "                val_acc / val_batches * 100))\n",
    "\n",
    "            # adjust learning rate as in paper\n",
    "            # 32k and 48k iterations should be roughly equivalent to 41 and 61 epochs\n",
    "            if (epoch+1) == 41 or (epoch+1) == 61:\n",
    "                new_lr = sh_lr.get_value() * 0.1\n",
    "                print(\"New LR:\"+str(new_lr))\n",
    "                sh_lr.set_value(lasagne.utils.floatX(new_lr))\n",
    "\n",
    "        # dump the network weights to a file :\n",
    "        np.savez('cifar10_deep_residual_model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    else:\n",
    "        # load network weights from model file\n",
    "        with np.load(model) as f:\n",
    "             param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "        lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "    # Calculate validation error of model:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, Y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    if ('--help' in sys.argv) or ('-h' in sys.argv):\n",
    "        print(\"Trains a Deep Residual Learning network on cifar-10 using Lasagne.\")\n",
    "        print(\"Network architecture and training parameters are as in section 4.2 in 'Deep Residual Learning for Image Recognition'.\")\n",
    "        print(\"Usage: %s [N [MODEL]]\" % sys.argv[0])\n",
    "        print()\n",
    "        print(\"N: Number of stacked residual building blocks per feature map (default: 5)\")\n",
    "        print(\"MODEL: saved model file to load (for validation) (default: None)\")\n",
    "    else:\n",
    "        kwargs = {}\n",
    "        if len(sys.argv) > 1:\n",
    "            kwargs['n'] = int(sys.argv[1])\n",
    "        if len(sys.argv) > 2:\n",
    "            kwargs['model'] = sys.argv[2]\n",
    "        main(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
